


import logging  
 import json  
 import traceback  
 from datetime import datetime  
 from typing import Dict, Any, List, Optional  
 from dataclasses import dataclass, asdict

 @dataclass  
 class DebuggingEntry:  
 	"""Represents a single debugging log entry"""  
 	timestamp: str  
 	task\_description: str  
 	agent\_response: str  
 	reasoning\_steps: List\[str\]  
 	success: bool  
 	error\_message: Optional\[str\] \= None  
 	validation\_results: Optional\[Dict\[str, Any\]\] \= None  
 	performance\_metrics: Optional\[Dict\[str, float\]\] \= None

 class AgentDebugger:  
 	"""Comprehensive debugging system for DeepSeek agent"""  
 	  
 	def \_\_init\_\_(self, log\_file: str \= "agent\_debug.log"):  
     	\# Set up detailed logging  
     	self.log\_file \= log\_file  
     	self.debug\_entries \= \[\]  
     	  
     	\# Configure logging with multiple outputs  
     	logging.basicConfig(  
         	level=logging.INFO,  
         	format='%(asctime)s \- %(levelname)s \- %(message)s',  
         	handlers=\[  
                 logging.FileHandler(log\_file, mode='a'),  
             	logging.StreamHandler()  
         	\]  
     	)  
     	  
     	self.logger \= logging.getLogger(\_\_name\_\_)  
     	self.logger.info("🐛 Agent Debugger initialized")  
     	  
     	print(f"✅ Debugging system active \- logs saved to: {log\_file}")





	def log\_agent\_interaction(self, task\_description: str, response: str,  
                             reasoning\_steps: List\[str\], success: bool \= True,  
                             error\_message: str \= None, \*\*kwargs) \-\> DebuggingEntry:  
     	"""Log a complete agent interaction with all details"""  
     	  
     	entry \= DebuggingEntry(  
             timestamp=datetime.now().isoformat(),  
             task\_description=task\_description,  
         	agent\_response=response,  
             reasoning\_steps=reasoning\_steps,  
         	success=success,  
         	error\_message=error\_message,  
             validation\_results=kwargs.get('validation\_results'),  
             performance\_metrics=kwargs.get('performance\_metrics')  
     	)  
     	  
     	self.debug\_entries.append(entry)  
     	  
     	\# Log to file with structured information  
     	self.logger.info(f"=== AGENT INTERACTION \===")  
     	self.logger.info(f"Task: {task\_description}")  
     	self.logger.info(f"Success: {success}")  
     	  
     	if not success and error\_message:  
             self.logger.error(f"Error: {error\_message}")  
     	  
     	self.logger.info(f"Response length: {len(response)} characters")  
     	self.logger.info(f"Reasoning steps found: {len(reasoning\_steps)}")  
     	  
     	for i, step in enumerate(reasoning\_steps, 1):  
         	self.logger.info(f"Step {i}: {step\[:100\]}{'...' if len(step) \> 100 else ''}")  
     	  
     	if kwargs.get('validation\_results'):  
             self.logger.info(f"Validation: {kwargs\['validation\_results'\]}")  
     	  
     	self.logger.info("=== END INTERACTION \===\\n")  
          
     	return entry  
 	  
 	def get\_debug\_summary(self) \-\> Dict\[str, Any\]:  
     	"""Get summary statistics of debugging sessions"""  
     	if not self.debug\_entries:  
         	return {"message": "No debugging entries found"}  
     	  
     	total\_entries \= len(self.debug\_entries)  
     	successful\_entries \= sum(1 for entry in self.debug\_entries if entry.success)  
     	failed\_entries \= total\_entries \- successful\_entries  
     	  
     	\# Calculate average reasoning steps  
     	avg\_reasoning\_steps \= sum(len(entry.reasoning\_steps) for entry in self.debug\_entries) / total\_entries  
     	  
     	\# Find common error patterns  
     	error\_messages \= \[entry.error\_message for entry in self.debug\_entries if entry.error\_message\]  
     	  
     	return {  
             "total\_interactions": total\_entries,  
             "successful\_interactions": successful\_entries,  
             "failed\_interactions": failed\_entries,  
         	"success\_rate": successful\_entries / total\_entries if total\_entries \> 0 else 0,  
             "average\_reasoning\_steps": avg\_reasoning\_steps,  
         	"common\_errors": list(set(error\_messages)) if error\_messages else \[\]  
     	}  
 	  
 	def export\_debug\_data(self, filename: str \= None) \-\> str:  
     	"""Export debugging data to JSON file"""  
     	if filename is None:  
         	filename \= f"debug\_export\_{datetime.now().strftime('%Y%m%d\_%H%M%S')}.json"  
     	  
     	export\_data \= {  
         	"export\_timestamp": datetime.now().isoformat(),  
         	"summary": self.get\_debug\_summary(),  
         	"entries": \[asdict(entry) for entry in self.debug\_entries\]  
     	}  
     	  
     	with open(filename, 'w') as f:  
         	json.dump(export\_data, f, indent=2)  
     	  
     	print(f"📁 Debug data exported to: {filename}")  
     	return filename





	def validate\_agent\_response(self, expected\_result: Any, actual\_result: str,  
                               validation\_type: str \= "exact\_match") \-\> Dict\[str, Any\]:  
     	"""Validate agent response against expected results"""  
     	  
     	validation\_result \= {  
         	"validation\_type": validation\_type,  
         	"expected": str(expected\_result),  
         	"actual": actual\_result,  
         	"passed": False,  
         	"details": ""  
     	}  
     	  
     	try:  
         	if validation\_type \== "exact\_match":  
                 validation\_result\["passed"\] \= str(expected\_result).strip().lower() \== actual\_result.strip().lower()  
             	  
         	elif validation\_type \== "contains":  
                 validation\_result\["passed"\] \= str(expected\_result).lower() in actual\_result.lower()  
             	  
         	elif validation\_type \== "numeric":  
             	\# Extract numbers from response for comparison  
             	import re  
             	actual\_numbers \= re.findall(r'-?\\d+(?:\\.\\d+)?', actual\_result)  
             	expected\_numbers \= re.findall(r'-?\\d+(?:\\.\\d+)?', str(expected\_result))  
             	  
             	if actual\_numbers and expected\_numbers:  
                     validation\_result\["passed"\] \= float(actual\_numbers\[-1\]) \== float(expected\_numbers\[-1\])  
                     validation\_result\["details"\] \= f"Expected number: {expected\_numbers\[-1\]}, Found: {actual\_numbers\[-1\] if actual\_numbers else 'None'}"  
             	else:  
                     validation\_result\["passed"\] \= False  
                     validation\_result\["details"\] \= "No numbers found in response"  
         	  
         	elif validation\_type \== "letter\_count":  
             	\# Special validation for letter counting problems  
             	parts \= str(expected\_result).split(':')  
             	if len(parts) \== 2:  
                 	word, letter \= parts\[0\].strip(), parts\[1\].strip()  
                 	actual\_count \= word.lower().count(letter.lower())  
                 	  
                 	\# Extract count from agent response  
                 	numbers \= re.findall(r'\\d+', actual\_result)  
                 	if numbers:  
                     	agent\_count \= int(numbers\[-1\])  
                         validation\_result\["passed"\] \= agent\_count \== actual\_count  
                         validation\_result\["details"\] \= f"Actual count: {actual\_count}, Agent count: {agent\_count}"  
                 	else:  
                         validation\_result\["passed"\] \= False  
                         validation\_result\["details"\] \= "No count found in agent response"  
         	  
         	if validation\_result\["passed"\]:  
             	self.logger.info(f"✅ Validation passed: {validation\_type}")  
         	else:  
                 self.logger.warning(f"❌ Validation failed: {validation\_type} \- {validation\_result\['details'\]}")  
             	  
     	except Exception as e:  
             validation\_result\["passed"\] \= False  
             validation\_result\["details"\] \= f"Validation error: {str(e)}"  
             self.logger.error(f"Validation exception: {e}")  
     	  
     	return validation\_result  
 	  
 	def assert\_agent\_accuracy(self, expected\_result: Any, actual\_result: str,  
                             validation\_type: str \= "exact\_match") \-\> bool:  
     	"""Assert that agent response meets expectations \- raises exception if not"""  
     	validation \= self.validate\_agent\_response(expected\_result, actual\_result, validation\_type)  
     	  
     	if not validation\["passed"\]:  
         	error\_msg \= f"Assertion failed: Expected '{validation\['expected'\]}', got '{validation\['actual'\]}'. {validation\['details'\]}"  
         	self.logger.error(error\_msg)  
         	raise AssertionError(error\_msg)  
     	  
     	return True





	def validate\_reasoning\_quality(self, reasoning\_steps: List\[str\]) \-\> Dict\[str, Any\]:  
     	"""Validate the quality of reasoning steps"""  
     	quality\_metrics \= {  
         	"total\_steps": len(reasoning\_steps),  
             "average\_step\_length": 0,  
         	"has\_logical\_flow": False,  
             "contains\_calculations": False,  
         	"has\_conclusion": False,  
         	"quality\_score": 0.0  
     	}  
     	  
     	if not reasoning\_steps:  
             quality\_metrics\["quality\_score"\] \= 0.0  
         	return quality\_metrics  
     	  
     	\# Calculate average step length  
     	total\_length \= sum(len(step) for step in reasoning\_steps)  
         quality\_metrics\["average\_step\_length"\] \= total\_length / len(reasoning\_steps)  
     	  
     	\# Check for logical flow indicators  
     	flow\_words \= \['first', 'next', 'then', 'therefore', 'because', 'since', 'so'\]  
     	combined\_text \= ' '.join(reasoning\_steps).lower()  
         quality\_metrics\["has\_logical\_flow"\] \= any(word in combined\_text for word in flow\_words)  
     	  
     	\# Check for calculations  
     	calculation\_indicators \= \['calculate', 'add', 'subtract', 'multiply', 'divide', 'equals', '='\]  
         quality\_metrics\["contains\_calculations"\] \= any(indicator in combined\_text for indicator in calculation\_indicators)  
     	  
     	\# Check for conclusion  
     	conclusion\_words \= \['therefore', 'thus', 'so', 'conclusion', 'final answer', 'result'\]  
         quality\_metrics\["has\_conclusion"\] \= any(word in combined\_text for word in conclusion\_words)  
     	  
     	\# Calculate overall quality score  
     	score \= 0  
     	score \+= min(quality\_metrics\["total\_steps"\] / 5, 1\) \* 0.3  \# Prefer 5+ steps  
     	score \+= (1 if quality\_metrics\["has\_logical\_flow"\] else 0\) \* 0.3  
     	score \+= (1 if quality\_metrics\["contains\_calculations"\] else 0\) \* 0.2  
     	score \+= (1 if quality\_metrics\["has\_conclusion"\] else 0\) \* 0.2  
     	  
         quality\_metrics\["quality\_score"\] \= score  
     	  
     	return quality\_metrics  
 	  
 	def detect\_common\_errors(self, response: str, reasoning\_steps: List\[str\]) \-\> List\[str\]:  
     	"""Detect common reasoning errors"""  
     	errors \= \[\]  
     	  
     	\# Check for contradictory statements  
     	if "yes" in response.lower() and "no" in response.lower():  
         	errors.append("Response contains contradictory statements")  
     	  
     	\# Check for incomplete reasoning  
     	if len(reasoning\_steps) \< 2:  
             errors.append("Insufficient reasoning steps (less than 2)")  
     	  
     	\# Check for circular reasoning  
     	for i, step in enumerate(reasoning\_steps):  
         	for j, other\_step in enumerate(reasoning\_steps):  
             	if i \!= j and step.lower() \== other\_step.lower():  
                     errors.append("Circular or repetitive reasoning detected")  
                 	break  
     	  
     	\# Check for unsupported conclusions  
     	conclusion\_words \= \['therefore', 'thus', 'so', 'conclusion'\]  
     	has\_conclusion \= any(word in response.lower() for word in conclusion\_words)  
 	    if has\_conclusion and len(reasoning\_steps) \< 3:  
             errors.append("Conclusion drawn without sufficient reasoning steps")  
     	  
     	return errors



**Step 3: Build Edge Case Testing System**

1\.   	**Create comprehensive test cases**:

o   Create a new file called agent\_test\_suite.py:



from deepseek\_agent import reasoning\_agent\_enhanced  
 from agent\_debugger import AgentDebugger  
 from reasoning\_extractor import ReasoningExtractor  
 from typing import List, Dict, Any

 class AgentTestSuite:  
 	"""Comprehensive test suite for DeepSeek agent"""  
 	  
 	def \_\_init\_\_(self):  
     	self.debugger \= AgentDebugger("test\_debug.log")  
     	self.extractor \= ReasoningExtractor()  
     	self.test\_results \= \[\]  
     	print("🧪 Agent Test Suite initialized")  
 	  
 	def run\_edge\_case\_tests(self) \-\> Dict\[str, Any\]:  
     	"""Run comprehensive edge case testing"""  
     	print("🎯 Running Edge Case Tests...")  
     	print("="\*50)  
     	  
     	edge\_cases \= \[  
         	\# Ambiguous questions  
         	{  
             	"name": "Ambiguous Question",  
             	"question": "How long is a piece of string?",  
                 "expected\_behavior": "should ask for clarification or explain ambiguity"  
         	},  
         	  
         	\# Missing information  
         	{  
             	"name": "Missing Information",  
             	"question": "Calculate the area of a rectangle with width 5.",  
                 "expected\_behavior": "should identify missing height/length"  
         	},  
         	  
         	\# Contradictory data  
         	{  
             	"name": "Contradictory Information",  
             	"question": "If all birds can fly, and penguins are birds, but penguins cannot fly, what's the conclusion?",  
                 "expected\_behavior": "should identify the logical contradiction"  
         	},  
         	  
         	\# Mathematical edge cases  
         	{  
                 "name": "Division by Zero",  
             	"question": "What is 10 divided by 0? Show your reasoning.",  
                 "expected\_behavior": "should explain why division by zero is undefined"  
         	},  
         	  
         	\# Very simple problems  
         	{  
             	"name": "Trivial Problem",  
             	"question": "What is 1 \+ 1?",  
                 "expected\_behavior": "should provide reasoning even for simple problems"  
         	},  
         	  
         	\# Complex multi-part problems  
         	{  
             	"name": "Complex Multi-part",  
             	"question": "A train travels 60 mph for 2 hours, then 80 mph for 1.5 hours. What's the average speed for the entire journey?",  
                 "expected\_behavior": "should break down into clear steps"  
         	}  
     	\]  
     	  
     	results \= \[\]  
     	  
     	for i, test\_case in enumerate(edge\_cases, 1):  
             print(f"\\n\[{i}/{len(edge\_cases)}\] Testing: {test\_case\['name'\]}")  
         	print(f"Question: {test\_case\['question'\]}")  
         	print(f"Expected: {test\_case\['expected\_behavior'\]}")  
         	print("-" \* 40\)  
         	  
         	try:  
             	\# Get agent response  
             	result \= reasoning\_agent\_enhanced.ask\_question(  
                     test\_case\['question'\],  
                 	show\_reasoning=False  
             	)  
             	  
             	if result\['status'\] \== 'success':  
                 	\# Extract reasoning steps  
                 	summary \= self.extractor.get\_reasoning\_summary(result\['response'\])  
                 	  
                 	\# Analyze the response  
                 	test\_result \= {  
                         "test\_name": test\_case\['name'\],  
                         "question": test\_case\['question'\],  
                         "response": result\['response'\],  
                         "reasoning\_steps": len(summary\['reasoning\_steps'\]),  
                         "quality\_score": summary\['quality\_metrics'\]\['quality\_score'\],  
                     	"errors\_detected": self.debugger.detect\_common\_errors(  
                             result\['response'\],  
                             \[step\['content'\] for step in summary\['reasoning\_steps'\]\]  
                     	),  
                         "passed": True  \# We'll evaluate this manually for edge cases  
                 	}  
                 	  
                 	\# Log the interaction  
                     self.debugger.log\_agent\_interaction(  
                         test\_case\['question'\],  
                     	result\['response'\],  
                     	\[step\['content'\] for step in summary\['reasoning\_steps'\]\],  
                     	success=True  
                 	)  
                 	  
                 	print(f"✅ Response received ({test\_result\['reasoning\_steps'\]} steps)")  
                 	print(f"Quality score: {test\_result\['quality\_score'\]:.2f}")  
                 	  
                 	if test\_result\['errors\_detected'\]:  
                     	print(f"⚠️ Errors detected: {', '.join(test\_result\['errors\_detected'\])}")  
                 	else:  
                     	print("✅ No obvious errors detected")  
                 	  
             	else:  
                 	test\_result \= {  
                         "test\_name": test\_case\['name'\],  
                         "question": test\_case\['question'\],  
                         "response": result\['response'\],  
                         "reasoning\_steps": 0,  
                         "quality\_score": 0.0,  
                         "errors\_detected": \["Agent request failed"\],  
                         "passed": False  
                 	}  
      	             
                 	print(f"❌ Agent failed: {result\['response'\]}")  
             	  
                 results.append(test\_result)  
             	  
         	except Exception as e:  
             	print(f"❌ Test exception: {str(e)}")  
             	results.append({  
                     "test\_name": test\_case\['name'\],  
                 	"question": test\_case\['question'\],  
                 	"response": f"Exception: {str(e)}",  
                     "reasoning\_steps": 0,  
                     "quality\_score": 0.0,  
                     "errors\_detected": \[f"Exception: {str(e)}"\],  
                 	"passed": False  
             	})  
     	  
     	\# Generate summary report  
     	total\_tests \= len(results)  
   	  successful\_tests \= sum(1 for r in results if r\['passed'\])  
     	avg\_quality \= sum(r\['quality\_score'\] for r in results) / total\_tests if total\_tests \> 0 else 0  
     	  
     	summary\_report \= {  
         	"total\_tests": total\_tests,  
         	"successful\_tests": successful\_tests,  
         	"success\_rate": successful\_tests / total\_tests if total\_tests \> 0 else 0,  
             "average\_quality\_score": avg\_quality,  
         	"detailed\_results": results  
     	}  
     	  
     	print(f"\\n{'='\*50}")  
     	print(f"🏆 EDGE CASE TEST SUMMARY:")  
     	print(f"   Tests run: {total\_tests}")  
     	print(f"   Success rate: {summary\_report\['success\_rate'\]:.2%}")  
     	print(f"   Average quality: {avg\_quality:.2f}")  
     	  
     	return summary\_report



2\.   	**Add specific validation tests**:

o   Continue in agent\_test\_suite.py:



	def run\_validation\_tests(self) \-\> Dict\[str, Any\]:  
     	"""Run tests with known correct answers for validation"""  
     	print("\\n🔍 Running Validation Tests...")  
     	print("="\*50)  
     	  
     	validation\_tests \= \[  
         	{  
             	"question": "How many R's are in the word 'strawberry'?",  
             	"expected": "strawberry:r",  \# Special format for letter counting  
                 "validation\_type": "letter\_count"  
         	},  
         	{  
             	"question": "What is 15% of 200?",  
             	"expected": "30",  
                 "validation\_type": "numeric"  
         	},  
         	{  
          	   "question": "If you buy 3 apples at $2 each and 2 oranges at $1.50 each, what's the total cost?",  
             	"expected": "9",  \# $6 \+ $3 \= $9  
                 "validation\_type": "numeric"  
         	},  
         	{  
             	"question": "Is Paris the capital of France?",  
             	"expected": "yes",  
                 "validation\_type": "contains"  
         	}  
     	\]  
     	  
     	validation\_results \= \[\]  
     	  
     	for i, test in enumerate(validation\_tests, 1):  
         	print(f"\\n\[{i}/{len(validation\_tests)}\] Validating: {test\['question'\]}")  
         	  
         	try:  
             	\# Get agent response  
             	result \= reasoning\_agent\_enhanced.ask\_question(test\['question'\], show\_reasoning=False)  
             	  
             	if result\['status'\] \== 'success':  
                 	\# Validate the response  
                 	validation \= self.debugger.validate\_agent\_response(  
                     	test\['expected'\],  
                         result\['response'\],  
                         test\['validation\_type'\]  
                 	)  
                 	  
                 	\# Extract reasoning for quality assessment  
                 	summary \= self.extractor.get\_reasoning\_summary(result\['response'\])  
                 	  
                 	validation\_result \= {  
                         "question": test\['question'\],  
                         "expected": test\['expected'\],  
                         "actual\_response": result\['response'\],  
                         "validation\_passed": validation\['passed'\],  
                         "validation\_details": validation\['details'\],  
                         "reasoning\_steps": len(summary\['reasoning\_steps'\]),  
                         "quality\_score": summary\['quality\_metrics'\]\['quality\_score'\]  
                 	}  
                 	  
                 	\# Log the result  
                     self.debugger.log\_agent\_interaction(  
                     	test\['question'\],  
                         result\['response'\],  
                     	\[step\['content'\] for step in summary\['reasoning\_steps'\]\],  
                         success=validation\['passed'\],  
                         validation\_results=validation  
                 	)  
                 	  
                 	status \= "✅ PASS" if validation\['passed'\] else "❌ FAIL"  
                 	print(f"   {status} \- {validation.get('details', 'No details')}")  
                 	  
             	else:  
                 	validation\_result \= {  
                         "question": test\['question'\],  
                         "expected": test\['expected'\],  
                         "actual\_response": result\['response'\],  
                         "validation\_passed": False,  
                         "validation\_details": "Agent request failed",  
                         "reasoning\_steps": 0,  
                         "quality\_score": 0.0  
                 	}  
                 	  
                 	print(f"   ❌ FAIL \- Agent error: {result\['response'\]}")  
             	  
                 validation\_results.append(validation\_result)  
             	  
         	except Exception as e:  
             	print(f"   ❌ EXCEPTION \- {str(e)}")  
                 validation\_results.append({  
                 	"question": test\['question'\],  
                 	"expected": test\['expected'\],  
                     "actual\_response": f"Exception: {str(e)}",  
                     "validation\_passed": False,  
                     "validation\_details": f"Test exception: {str(e)}",  
                     "reasoning\_steps": 0,  
                     "quality\_score": 0.0  
             	})  
     	  
     	\# Calculate summary statistics  
     	total\_validations \= len(validation\_results)  
     	passed\_validations \= sum(1 for r in validation\_results if r\['validation\_passed'\])  
     	avg\_quality \= sum(r\['quality\_score'\] for r in validation\_results) / total\_validations if total\_validations \> 0 else 0  
     	  
     	validation\_summary \= {  
         	"total\_tests": total\_validations,  
         	"passed\_tests": passed\_validations,  
             "validation\_accuracy": passed\_validations / total\_validations if total\_validations \> 0 else 0,  
         	"average\_quality\_score": avg\_quality,  
         	"detailed\_results": validation\_results  
     	}  
     	  
     	print(f"\\n📊 VALIDATION TEST SUMMARY:")  
     	print(f"   Accuracy: {validation\_summary\['validation\_accuracy'\]:.2%} ({passed\_validations}/{total\_validations})")  
     	print(f"   Average quality: {avg\_quality:.2f}")  
     	  
     	return validation\_summary






import json  
 from datetime import datetime  
 from typing import Dict, List, Any  
 from agent\_debugger import AgentDebugger

 class ImprovementTracker:  
 	"""Tracks agent improvements over time"""  
 	  
 	def \_\_init\_\_(self, history\_file: str \= "improvement\_history.json"):  
     	self.history\_file \= history\_file  
     	self.improvement\_history \= self.load\_history()  
     	print("📈 Improvement Tracker initialized")  
 	  
 	def load\_history(self) \-\> List\[Dict\[str, Any\]\]:  
     	"""Load improvement history from file"""  
     	try:  
         	with open(self.history\_file, 'r') as f:  
             	return json.load(f)  
     	except FileNotFoundError:  
         	return \[\]  
     	except json.JSONDecodeError:  
         	print(f"⚠️ Warning: Could not parse {self.history\_file}, starting fresh")  
         	return \[\]  
 	  
 	def save\_history(self):  
     	"""Save improvement history to file"""  
     	with open(self.history\_file, 'w') as f:  
             json.dump(self.improvement\_history, f, indent=2)  
 	  
 	def record\_improvement\_session(self, test\_results: Dict\[str, Any\],  
                                  improvements\_made: List\[str\] \= None):  
     	"""Record the results of an improvement session"""  
     	  
     	session\_record \= {  
         	"timestamp": datetime.now().isoformat(),  
         	"test\_summary": {  
             	"total\_tests": test\_results.get('total\_tests', 0),  
             	"success\_rate": test\_results.get('success\_rate', 0),  
                 "validation\_accuracy": test\_results.get('validation\_accuracy', 0),  
                 "average\_quality\_score": test\_results.get('average\_quality\_score', 0\)  
         	},  
             "improvements\_made": improvements\_made or \[\],  
         	"notes": ""  
     	}  
     	  
         self.improvement\_history.append(session\_record)  
     	self.save\_history()  
     	  
     	print(f"📝 Improvement session recorded")  
     	  
 	def analyze\_improvement\_trends(self) \-\> Dict\[str, Any\]:  
     	"""Analyze improvement trends over time"""  
     	if len(self.improvement\_history) \< 2:  
         	return {"message": "Need at least 2 sessions to analyze trends"}  
     	  
     	\# Get latest and previous sessions  
     	latest \= self.improvement\_history\[-1\]\['test\_summary'\]  
     	previous \= self.improvement\_history\[-2\]\['test\_summary'\]  
     	  
     	trends \= {  
         	"success\_rate\_change": latest\['success\_rate'\] \- previous\['success\_rate'\],  
         	"accuracy\_change": latest.get('validation\_accuracy', 0\) \- previous.get('validation\_accuracy', 0),  
         	"quality\_change": latest\['average\_quality\_score'\] \- previous\['average\_quality\_score'\],  
         	"total\_sessions": len(self.improvement\_history),  
             "improvements\_over\_time": \[session\['improvements\_made'\] for session in self.improvement\_history\]  
     	}  
     	  
     	\# Determine overall trend  
     	improvements \= sum(\[  
         	1 if trends\['success\_rate\_change'\] \> 0 else 0,  
         	1 if trends\['accuracy\_change'\] \> 0 else 0,  
         	1 if trends\['quality\_change'\] \> 0 else 0  
     	\])  
     	  
     	if improvements \>= 2:  
         	trends\['overall\_trend'\] \= "improving"  
     	elif improvements \>= 1:  
         	trends\['overall\_trend'\] \= "mixed"  
     	else:  
         	trends\['overall\_trend'\] \= "declining"  
     	  
     	return trends  
 	  
 	def suggest\_improvements(self, test\_results: Dict\[str, Any\]) \-\> List\[str\]:  
     	"""Suggest specific improvements based on test results"""  
     	suggestions \= \[\]  
     	  
     	\# Based on success rate  
     	if test\_results.get('success\_rate', 1\) \< 0.8:  
             suggestions.append("Consider revising prompt templates for better clarity")  
         	suggestions.append("Add more specific instructions for reasoning steps")  
     	  
     	\# Based on validation accuracy  
     	if test\_results.get('validation\_accuracy', 1\) \< 0.8:  
         	suggestions.append("Implement more robust validation checks")  
         	suggestions.append("Add specific examples for common problem types")  
     	  
     	\# Based on quality scores  
     	if test\_results.get('average\_quality\_score', 1\) \< 0.7:  
             suggestions.append("Encourage more detailed step-by-step explanations")  
         	suggestions.append("Add prompts that specifically ask for reasoning justification")  
     	  
     	\# Based on historical trends  
     	trends \= self.analyze\_improvement\_trends()  
         if isinstance(trends, dict) and trends.get('overall\_trend') \== 'declining':  
             suggestions.append("Review recent changes that may have negatively impacted performance")  
             suggestions.append("Consider reverting to a previous configuration")  
     	  
     	return suggestions






from agent\_test\_suite import AgentTestSuite  
 from improvement\_tracker import ImprovementTracker  
 from agent\_debugger import AgentDebugger  
 import json

 def run\_comprehensive\_debugging\_session():  
 	"""Run a complete debugging and improvement session"""  
 	print("🔧 COMPREHENSIVE DEBUGGING SESSION")  
 	print("="\*60)  
 	print("This will test your agent thoroughly and suggest improvements.")  
 	print("-"\*60)  
 	  
 	\# Initialize systems  
 	test\_suite \= AgentTestSuite()  
 	improvement\_tracker \= ImprovementTracker()  
 	debugger \= AgentDebugger("comprehensive\_debug.log")  
 	  
 	\# Run all tests  
 	print("\\n🎯 Phase 1: Edge Case Testing")  
 	edge\_case\_results \= test\_suite.run\_edge\_case\_tests()  
 	  
 	print("\\n🔍 Phase 2: Validation Testing")  
 	validation\_results \= test\_suite.run\_validation\_tests()  
 	  
 	\# Combine results  
 	comprehensive\_results \= {  
     	"edge\_case\_results": edge\_case\_results,  
     	"validation\_results": validation\_results,  
     	"combined\_metrics": {  
             "overall\_success\_rate": (edge\_case\_results.get('success\_rate', 0\) \+  
                                    validation\_results.get('validation\_accuracy', 0)) / 2,  
             "combined\_quality\_score": (edge\_case\_results.get('average\_quality\_score', 0\) \+  
                                      validation\_results.get('average\_quality\_score', 0)) / 2  
     	}  
 	}  
 	  
 	\# Get improvement suggestions  
 	print("\\n💡 Phase 3: Improvement Analysis")  
 	suggestions \= improvement\_tracker.suggest\_improvements(comprehensive\_results\['combined\_metrics'\])  
 	  
 	print("📋 IMPROVEMENT SUGGESTIONS:")  
 	if suggestions:  
     	for i, suggestion in enumerate(suggestions, 1):  
         	print(f"   {i}. {suggestion}")  
 	else:  
     	print("   🎉 No major improvements needed\! Your agent is performing well.")  
 	  
 	\# Get debugging summary  
 	debug\_summary \= debugger.get\_debug\_summary()  
 	  
 	\# Show comprehensive report  
 	print(f"\\n{'='\*60}")  
 	print("📊 COMPREHENSIVE DEBUGGING REPORT")  
 	print("="\*60)  
 	  
 	print(f"🎯 Edge Case Performance:")  
 	print(f"   Success Rate: {edge\_case\_results.get('success\_rate', 0):.2%}")  
 	print(f"   Average Quality: {edge\_case\_results.get('average\_quality\_score', 0):.2f}")  
 	  
 	print(f"\\n🔍 Validation Performance:")  
 	print(f"   Accuracy: {validation\_results.get('validation\_accuracy', 0):.2%}")  
 	print(f"   Quality Score: {validation\_results.get('average\_quality\_score', 0):.2f}")  
 	  
 	print(f"\\n📈 Overall Metrics:")  
 	print(f"   Combined Success Rate: {comprehensive\_results\['combined\_metrics'\]\['overall\_success\_rate'\]:.2%}")  
 	print(f"   Combined Quality Score: {comprehensive\_results\['combined\_metrics'\]\['combined\_quality\_score'\]:.2f}")  
 	  
 	print(f"\\n🐛 Debug Statistics:")  
 	print(f"   Total Interactions: {debug\_summary.get('total\_interactions', 0)}")  
 	print(f"   Success Rate: {debug\_summary.get('success\_rate', 0):.2%}")  
 	print(f"   Average Reasoning Steps: {debug\_summary.get('average\_reasoning\_steps', 0):.1f}")  
 	  
 	\# Record this session  
     improvement\_tracker.record\_improvement\_session(  
         comprehensive\_results\['combined\_metrics'\],  
     	improvements\_made=\[\]  \# User can fill this in manually  
 	)  
 	  
 	\# Export detailed results  
 	export\_filename \= debugger.export\_debug\_data()  
 	  
 	print(f"\\n💾 Session Results:")  
 	print(f"   Detailed debug log: comprehensive\_debug.log")  
 	print(f"   Exported data: {export\_filename}")  
 	print(f"   Improvement history: improvement\_history.json")  
 	  
 	print(f"\\n🎉 Comprehensive debugging session completed\!")  
 	  
 	return comprehensive\_results

 def interactive\_debugging\_menu():  
 	"""Interactive menu for debugging options"""  
 	while True:  
     	print("\\n🔧 DEBUGGING MENU")  
     	print("="\*30)  
     	print("1. Run comprehensive debugging session")  
     	print("2. Run edge case tests only")  
     	print("3. Run validation tests only")  
     	print("4. View improvement history")  
     	print("5. Export debug data")  
     	print("6. Exit")  
     	  
     	choice \= input("\\nSelect option (1-6): ").strip()  
     	  
     	if choice \== "1":  
             run\_comprehensive\_debugging\_session()  
     	elif choice \== "2":  
         	test\_suite \= AgentTestSuite()  
             test\_suite.run\_edge\_case\_tests()  
     	elif choice \== "3":  
         	test\_suite \= AgentTestSuite()  
             test\_suite.run\_validation\_tests()  
     	elif choice \== "4":  
         	tracker \= ImprovementTracker()  
         	trends \= tracker.analyze\_improvement\_trends()  
         	print(f"\\n📈 Improvement Trends: {json.dumps(trends, indent=2)}")  
     	elif choice \== "5":  
         	debugger \= AgentDebugger()  
         	filename \= debugger.export\_debug\_data()  
         	print(f"✅ Data exported to: {filename}")  
     	elif choice \== "6":  
         	print("👋 Goodbye\! Happy debugging\!")  
         	break  
     	else:  
         	print("❌ Invalid option. Please select 1-6.")

 if \_\_name\_\_ \== "\_\_main\_\_":  
 	interactive\_debugging\_menu()





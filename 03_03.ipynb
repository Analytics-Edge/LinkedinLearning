


pip install requests






import requests  
 import json  
 import time

 def call\_deepseek\_api(prompt, config):  
 	"""  
 	Make a direct API call to DeepSeek using standard HTTP requests.  
 	This function handles the complete request/response cycle.  
 	"""  
 	  
 	\# Set up the request headers  
 	\# These tell the API what format we're sending and provide authentication  
 	headers \= {  
     	"Content-Type": "application/json",  
     	"Authorization": f"Bearer {config\['api\_key'\]}"  
 	}  
 	  
 	\# Create the request payload with all necessary parameters  
 	payload \= {  
     	"model": config\["model"\],  
     	"messages": \[{"role": "user", "content": prompt}\],  
     	"temperature": config\["temperature"\],  
     	"max\_tokens": config\["max\_tokens"\],  
     	"reasoning\_steps": config.get("reasoning\_steps", True)  
 	}  
 	  
 	try:  
     	\# Make the HTTP POST request to the DeepSeek API  
     	print(f"🔄 Sending request to DeepSeek API...")  
     	response \= requests.post(  
             f"{config\['base\_url'\]}/chat/completions",  
         	headers=headers,  
         	json=payload,  
         	timeout=30  \# 30 second timeout to prevent hanging  
     	)  
     	  
     	\# Check if the request was successful  
     	if response.status\_code \== 200:  
         	print("✅ API request successful")  
         	return response.json()  
     	else:  
         	print(f"❌ API request failed with status code: {response.status\_code}")  
         	print(f"Error response: {response.text}")  
         	return None  
         	  
 	except requests.exceptions.Timeout:  
     	print("❌ Request timed out after 30 seconds")  
     	return None  
 	except requests.exceptions.RequestException as e:  
     	print(f"❌ Request failed: {str(e)}")  
     	return None






def test\_api\_integration(config):  
 	"""  
 	Test the DeepSeek API integration with a simple reasoning task.  
 	"""  
 	print("🧪 Testing DeepSeek API integration...")  
 	  
 	\# Simple test prompt that should trigger reasoning  
 	test\_prompt \= "If a train travels 60 miles per hour for 2.5 hours, how far does it travel? Please show your reasoning steps."  
 	  
 	\# Make the API call  
 	result \= call\_deepseek\_api(test\_prompt, config)  
 	  
 	if result:  
     	print("✅ API integration test successful\!")  
     	print("\\n📋 Response preview:")  
     	  
     	\# Extract and display the response content  
     	if 'choices' in result and len(result\['choices'\]) \> 0:  
         	content \= result\['choices'\]\[0\]\['message'\]\['content'\]  
         	print(content\[:200\] \+ "..." if len(content) \> 200 else content)  
     	  
     	\# Display usage information if available  
     	if 'usage' in result:  
         	print(f"\\n📊 Token usage: {result\['usage'\]\['total\_tokens'\]} tokens")  
 	else:  
     	print("❌ API integration test failed")  
     	print("Please check your configuration and API key")

 \# Test your integration (assuming you have your config from the previous chapter)  
 \# test\_api\_integration(deepseek\_config)






pip install praisonaiagents






from praisonaiagents import Agent

 def create\_deepseek\_agent(config):  
 	"""  
 	Create a PraisonAI agent configured to use DeepSeek's reasoning capabilities.  
 	"""  
 	  
 	\# Create the agent with DeepSeek configuration  
 	agent \= Agent(  
     	role="Reasoning Specialist",  
     	goal="Provide clear, step-by-step analysis and solutions",  
     	backstory="An expert at transparent reasoning who always shows their thinking process",  
     	llm\_config=config  \# Your DeepSeek configuration from the previous chapter  
 	)  
 	  
 	print("✅ DeepSeek reasoning agent created successfully\!")  
 	return agent

 def test\_praisonai\_integration(agent):  
 	"""  
 	Test the PraisonAI integration with a reasoning task.  
 	"""  
 	print("🧪 Testing PraisonAI integration with DeepSeek...")  
 	  
 	test\_prompt \= "A company's revenue increased by 25% in Q1 and then decreased by 15% in Q2. If the original revenue was $100,000, what's the final revenue after Q2? Show your reasoning."  
 	  
 	try:  
     	\# Use the agent to process the prompt  
     	response \= agent.execute\_task(test\_prompt)  
     	  
     	print("✅ PraisonAI integration successful\!")  
     	print(f"\\n📋 Agent response:\\n{response}")  
     	  
 	except Exception as e:  
     	print(f"❌ PraisonAI integration failed: {str(e)}")

 \# Example usage:  
 \# deepseek\_agent \= create\_deepseek\_agent(deepseek\_config)  
 \# test\_praisonai\_integration(deepseek\_agent)






import requests  
 import json  
 from typing import Dict, Optional, Union

 class DeepSeekIntegration:  
 	"""  
 	A flexible integration layer for DeepSeek API that supports multiple approaches.  
 	"""  
 	  
 	def \_\_init\_\_(self, config: Dict):  
     	"""Initialize the integration with configuration."""  
     	self.config \= config  
     	self.validate\_config()  
     	  
 	def validate\_config(self):  
     	"""Validate the configuration before use."""  
     	required\_fields \= \["api\_key", "base\_url", "model"\]  
     	for field in required\_fields:  
         	if not self.config.get(field):  
             	raise ValueError(f"Missing required configuration field: {field}")  
 	  
 	def make\_direct\_request(self, prompt: str, \*\*kwargs) \-\> Optional\[Dict\]:  
     	"""  
     	Make a direct HTTP request to the DeepSeek API.  
     	Additional parameters can be passed via kwargs.  
     	"""  
     	  
     	\# Merge any additional parameters with base config  
     	request\_config \= self.config.copy()  
     	request\_config.update(kwargs)  
     	  
     	headers \= {  
         	"Content-Type": "application/json",  
         	"Authorization": f"Bearer {request\_config\['api\_key'\]}"  
     	}  
     	  
     	payload \= {  
         	"model": request\_config\["model"\],  
         	"messages": \[{"role": "user", "content": prompt}\],  
         	"temperature": request\_config.get("temperature", 0.1),  
         	"max\_tokens": request\_config.get("max\_tokens", 3000),  
         	"reasoning\_steps": request\_config.get("reasoning\_steps", True)  
     	}  
     	  
     	try:  
         	print(f"🔄 Making direct API request...")  
         	response \= requests.post(  
                 f"{request\_config\['base\_url'\]}/chat/completions",  
             	headers=headers,  
             	json=payload,  
             	timeout=30  
         	)  
         	  
         	if response.status\_code \== 200:  
             	print("✅ Direct request successful")  
             	return response.json()  
         	else:  
             	print(f"❌ Request failed: {response.status\_code}")  
             	return None  
             	  
     	except Exception as e:  
         	print(f"❌ Request error: {str(e)}")  
         	return None  
 	  
 	def create\_agent(self, role: str \= "AI Assistant",  
                 	goal: str \= "Provide helpful responses",  
                 	backstory: str \= "A helpful AI assistant"):  
     	"""  
     	Create a PraisonAI agent with the current configuration.  
     	"""  
     	try:  
         	from praisonaiagents import Agent  
         	  
         	agent \= Agent(  
             	role=role,  
             	goal=goal,  
             	backstory=backstory,  
             	llm\_config=self.config  
         	)  
         	  
         	print(f"✅ Created agent: {role}")  
         	return agent  
         	  
     	except ImportError:  
         	print("❌ PraisonAI not available. Install with: pip install praisonaiagents")  
         	return None  
     	except Exception as e:  
         	print(f"❌ Agent creation failed: {str(e)}")  
         	return None  
 	  
 	def ask(self, prompt: str, method: str \= "direct", \*\*kwargs) \-\> Optional\[str\]:  
     	"""  
     	Ask a question using the specified method (direct or agent).  
     	"""  
     	if method \== "direct":  
         	result \= self.make\_direct\_request(prompt, \*\*kwargs)  
         	if result and 'choices' in result:  
             	return result\['choices'\]\['message'\]\['content'\]  
         	return None  
         	  
     	elif method \== "agent":  
         	agent \= self.create\_agent(\*\*kwargs)  
         	if agent:  
             	try:  
                 	return agent.execute\_task(prompt)  
             	except Exception as e:  
                 	print(f"❌ Agent execution failed: {str(e)}")  
                 	return None  
         	return None  
         	  
     	else:  
         	print(f"❌ Unknown method: {method}. Use 'direct' or 'agent'")  
         	return None  
 	  
 	def benchmark\_methods(self, test\_prompt: str):  
     	"""  
     	Compare the performance of different integration methods.  
     	"""  
     	print("🏁 Benchmarking integration methods...")  
     	  
     	results \= {}  
     	  
     	\# Test direct method  
     	print("\\n📊 Testing direct API method...")  
     	start\_time \= time.time()  
     	direct\_result \= self.ask(test\_prompt, method="direct")  
     	direct\_time \= time.time() \- start\_time  
     	  
     	results\['direct'\] \= {  
         	'time': direct\_time,  
         	'success': direct\_result is not None,  
         	'response\_length': len(direct\_result) if direct\_result else 0  
     	}  
     	  
     	\# Test agent method  
     	print("\\n📊 Testing PraisonAI agent method...")  
     	start\_time \= time.time()  
     	agent\_result \= self.ask(test\_prompt, method="agent")  
     	agent\_time \= time.time() \- start\_time  
     	  
     	results\['agent'\] \= {  
         	'time': agent\_time,  
         	'success': agent\_result is not None,  
         	'response\_length': len(agent\_result) if agent\_result else 0  
     	}  
     	  
     	\# Display benchmark results  
     	print("\\n📈 Benchmark Results:")  
     	for method, data in results.items():  
         	status \= "✅ Success" if data\['success'\] else "❌ Failed"  
         	print(f"{method.title()} method: {status}, {data\['time'\]:.2f}s, {data\['response\_length'\]} chars")  
     	  
     	return results

 \# Example usage  
 def demonstrate\_integration\_flexibility():  
 	"""  
 	Demonstrate the flexibility of the integration layer.  
 	"""  
 	\# Assuming you have your config from previous chapters  
 	try:  
     	\# Create the integration instance  
     	integration \= DeepSeekIntegration(deepseek\_config)  
     	  
     	\# Test prompt  
     	test\_prompt \= "What are the key advantages of using renewable energy sources?"  
     	  
     	print("🔄 Demonstrating integration flexibility...\\n")  
     	  
     	\# Method 1: Direct API call  
     	print("Method 1: Direct API Call")  
     	direct\_response \= integration.ask(test\_prompt, method="direct")  
     	if direct\_response:  
         	print(f"Response: {direct\_response\[:100\]}...")  
     	  
     	print("\\n" \+ "="\*50 \+ "\\n")  
     	  
     	\# Method 2: Agent-based approach  
     	print("Method 2: Agent-based Approach")  
     	agent\_response \= integration.ask(  
         	test\_prompt,  
     	    method="agent",  
         	role="Environmental Expert",  
         	goal="Provide comprehensive information about renewable energy"  
     	)  
     	if agent\_response:  
         	print(f"Response: {agent\_response\[:100\]}...")  
     	  
     	print("\\n" \+ "="\*50 \+ "\\n")  
     	  
     	\# Method 3: Benchmark comparison  
         integration.benchmark\_methods(test\_prompt)  
     	  
 	except NameError:  
     	print("❌ Please ensure your deepseek\_config is defined from previous chapters")

 \# Uncomment to run the demonstration  
 \# demonstrate\_integration\_flexibility()






def debug\_api\_call(prompt, config, save\_to\_file=True):  
 	"""  
 	Make an API call with detailed debugging information.  
 	"""  
 	print("🔍 Debug mode: Making API call with full logging...")  
 	  
 	\# Log the request details  
 	print(f"📤 Request URL: {config\['base\_url'\]}/chat/completions")  
 	print(f"📤 Model: {config\['model'\]}")  
 	print(f"📤 Temperature: {config\['temperature'\]}")  
 	print(f"📤 Max Tokens: {config\['max\_tokens'\]}")  
 	print(f"📤 Prompt: {prompt\[:100\]}...")  
 	  
 	\# Make the call and capture timing  
 	start\_time \= time.time()  
 	result \= call\_deepseek\_api(prompt, config)  
 	end\_time \= time.time()  
 	  
 	print(f"⏱️ Request took: {end\_time \- start\_time:.2f} seconds")  
 	  
 	if result:  
     	print("📥 Response received successfully")  
     	if 'usage' in result:  
         	usage \= result\['usage'\]  
         	print(f"📊 Tokens used: {usage.get('total\_tokens', 'unknown')}")  
     	  
     	\# Optionally save to file for detailed analysis  
     	if save\_to\_file:  
         	debug\_filename \= f"debug\_response\_{int(time.time())}.json"  
         	with open(debug\_filename, 'w') as f:  
             	json.dump(result, f, indent=2)  
         	print(f"💾 Full response saved to: {debug\_filename}")  
 	else:  
     	print("❌ No response received")  
 	  
 	return result






import unittest  
 from unittest.mock import Mock, patch

 class TestDeepSeekIntegration(unittest.TestCase):  
 	"""  
 	Unit tests for DeepSeek integration.  
 	"""  
 	  
 	def setUp(self):  
     	"""Set up test configuration."""  
     	self.test\_config \= {  
         	'api\_key': 'test\_key\_123',  
         	'base\_url': 'https://api.deepseek.com/v1',  
         	'model': 'deepseek-reasoner',  
         	'temperature': 0.1,  
         	'max\_tokens': 1000  
     	}  
 	  
 	@patch('requests.post')  
 	def test\_successful\_api\_call(self, mock\_post):  
     	"""Test a successful API call."""  
     	\# Mock a successful response  
     	mock\_response \= Mock()  
     	mock\_response.status\_code \= 200  
     	mock\_response.json.return\_value \= {  
         	'choices': \[{'message': {'content': 'Test response'}}\],  
         	'usage': {'total\_tokens': 50}  
     	}  
     	mock\_post.return\_value \= mock\_response  
     	  
     	\# Make the call  
     	result \= call\_deepseek\_api("Test prompt", self.test\_config)  
     	  
         \# Verify the result  
     	self.assertIsNotNone(result)  
     	self.assertIn('choices', result)  
     	  
 	@patch('requests.post')  
 	def test\_api\_error\_handling(self, mock\_post):  
     	"""Test API error handling."""  
     	\# Mock a failed response  
     	mock\_response \= Mock()  
     	mock\_response.status\_code \= 401  
     	mock\_response.text \= "Unauthorized"  
     	mock\_post.return\_value \= mock\_response  
     	  
     	\# Make the call  
     	result \= call\_deepseek\_api("Test prompt", self.test\_config)  
     	  
     	\# Verify error handling  
     	self.assertIsNone(result)

 \# Run tests with: python \-m pytest test\_file.py






import requests  
 import json  
 import time  
 from typing import Optional, Dict

 class CompleteDeepSeekIntegration:  
 	"""  
 	A complete DeepSeek integration example showcasing all approaches.  
 	"""  
 	  
 	def \_\_init\_\_(self, config: Dict):  
     	self.config \= config  
     	self.request\_history \= \[\]  
 	  
 	def direct\_api\_request(self, prompt: str) \-\> Optional\[Dict\]:  
     	"""Direct API integration approach."""  
     	headers \= {  
         	"Content-Type": "application/json",  
         	"Authorization": f"Bearer {self.config\['api\_key'\]}"  
     	}  
     	  
     	payload \= {  
         	"model": self.config\["model"\],  
         	"messages": \[{"role": "user", "content": prompt}\],  
         	"temperature": self.config\["temperature"\],  
         	"max\_tokens": self.config\["max\_tokens"\],  
         	"reasoning\_steps": self.config.get("reasoning\_steps", True)  
     	}  
     	  
     	try:  
         	response \= requests.post(  
                 f"{self.config\['base\_url'\]}/chat/completions",  
           	  headers=headers,  
             	json=payload,  
             	timeout=30  
         	)  
         	  
         	\# Log the request for analytics  
         	self.request\_history.append({  
             	'timestamp': time.time(),  
             	'method': 'direct',  
             	'prompt\_length': len(prompt),  
             	'success': response.status\_code \== 200  
         	})  
         	  
         	return response.json() if response.status\_code \== 200 else None  
         	  
     	except Exception as e:  
         	print(f"Direct API error: {e}")  
         	return None  
 	  
 	def framework\_request(self, prompt: str, role: str \= "AI Assistant") \-\> Optional\[str\]:  
     	"""Framework-based integration approach."""  
     	try:  
         	from praisonaiagents import Agent  
         	  
         	agent \= Agent(  
             	role=role,  
             	goal="Provide clear, helpful responses",  
             	backstory="A knowledgeable AI assistant",  
             	llm\_config=self.config  
         	)  
         	  
         	result \= agent.execute\_task(prompt)  
         	  
         	\# Log the request  
         	self.request\_history.append({  
             	'timestamp': time.time(),  
             	'method': 'framework',  
             	'prompt\_length': len(prompt),  
             	'success': result is not None  
         	})  
         	  
         	return result  
         	  
     	except ImportError:  
         	print("PraisonAI not available. Using direct method as fallback.")  
         	direct\_result \= self.direct\_api\_request(prompt)  
         	if direct\_result and 'choices' in direct\_result:  
             	return direct\_result\['choices'\]\[0\]\['message'\]\['content'\]  
         	return None  
     	except Exception as e:  
         	print(f"Framework error: {e}")  
         	return None  
 	  
 	def smart\_request(self, prompt: str, prefer\_method: str \= "auto") \-\> str:  
     	"""  
     	Intelligent request routing that chooses the best method automatically.  
     	"""  
     	  
     	\# Simple heuristic: use framework for complex prompts, direct for simple ones  
     	if prefer\_method \== "auto":  
         	if len(prompt) \> 200 or "analyze" in prompt.lower() or "explain" in prompt.lower():  
             	method \= "framework"  
         	else:  
             	method \= "direct"  
     	else:  
         	method \= prefer\_method  
     	  
     	print(f"🧠 Using {method} method for this request")  
     	  
     	if method \== "framework":  
         	result \= self.framework\_request(prompt)  
         	if result:  
             	return result  
         	\# Fallback to direct if framework fails  
         	print("⚠️ Falling back to direct method")  
     	  
     	\# Use direct method  
     	direct\_result \= self.direct\_api\_request(prompt)  
     	if direct\_result and 'choices' in direct\_result:  
         	return direct\_result\['choices'\]\['message'\]\['content'\]  
     	  
     	return "Sorry, I couldn't process your request at this time."  
 	  
 	def get\_analytics(self) \-\> Dict:  
     	"""Get analytics about integration usage."""  
     	if not self.request\_history:  
         	return {"message": "No requests made yet"}  
     	  
     	total\_requests \= len(self.request\_history)  
     	successful\_requests \= sum(1 for req in self.request\_history if req\['success'\])  
     	  
     	method\_counts \= {}  
     	for req in self.request\_history:  
         	method \= req\['method'\]  
         	method\_counts\[method\] \= method\_counts.get(method, 0\) \+ 1  
     	  
     	return {  
         	"total\_requests": total\_requests,  
             "successful\_requests": successful\_requests,  
         	"success\_rate": f"{(successful\_requests/total\_requests)\*100:.1f}%",  
         	"method\_usage": method\_counts,  
             "average\_prompt\_length": sum(req\['prompt\_length'\] for req in self.request\_history) / total\_requests  
     	}

 \# Complete demonstration function  
 def run\_complete\_demo():  
 	"""  
 	Run a complete demonstration of all integration approaches.  
 	"""  
 	print("🚀 Starting Complete DeepSeek Integration Demo\\n")  
 	  
 	\# Sample configuration (replace with your actual config)  
 	demo\_config \= {  
     	"model": "deepseek-reasoner",  
     	"api\_key": "your\_api\_key\_here",  \# Replace with actual key  
     	"base\_url": "https://api.deepseek.com/v1",  
     	"temperature": 0.1,  
     	"max\_tokens": 2000,  
     	"reasoning\_steps": True  
 	}  
 	  
 	\# Create integration instance  
 	integration \= CompleteDeepSeekIntegration(demo\_config)  
 	  
 	\# Test prompts  
 	test\_prompts \= \[  
     	"What is 2+2?",  \# Simple prompt  
     	"Analyze the benefits and drawbacks of remote work for companies and employees.",  \# Complex prompt  
     	"Explain quantum computing in simple terms."  \# Medium complexity  
 	\]  
 	  
 	print("📝 Testing different prompts with smart routing:\\n")  
 	  
 	for i, prompt in enumerate(test\_prompts, 1):  
     	print(f"Test {i}: {prompt}")  
     	response \= integration.smart\_request(prompt)  
     	print(f"Response: {response\[:150\]}...\\n")  
     	time.sleep(1)  \# Be respectful to the API  
 	  
 	\# Display analytics  
 	print("📊 Integration Analytics:")  
 	analytics \= integration.get\_analytics()  
 	for key, value in analytics.items():  
     	print(f"  {key}: {value}")

 \# Uncomment to run the demo (make sure to set your actual API key first)  
 \# run\_complete\_demo()





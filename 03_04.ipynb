








import random  
 import hashlib  
 import time

 class ABTestRouter:  
 	def \_\_init\_\_(self):  
     	self.treatment\_percentage \= 0.05  \# 5% to reasoning agent  
     	self.results \= \[\]  
     	  
 	def get\_user\_group(self, user\_id):  
     	\# Consistent assignment based on user ID hash  
     	hash\_value \= int(hashlib.md5(user\_id.encode()).hexdigest()\[:8\], 16\)  
     	normalized \= hash\_value / 0xFFFFFFFF  
     	  
     	return "treatment" if normalized \< self.treatment\_percentage else "control"  
 	  
 	def call\_baseline\_model(self, query):  
     	\# Placeholder for your baseline model \- replace with actual implementation  
     	time.sleep(0.5)  \# Simulate processing time  
     	return f"Baseline model response: {query\[:30\]}..."  
 	  
 	def call\_reasoning\_agent(self, query):  
     	\# Placeholder for your DeepSeek agent \- replace with actual implementation  
     	time.sleep(1.2)  \# DeepSeek typically takes longer due to reasoning  
     	return f"DeepSeek reasoning response: {query\[:30\]}... \[with step-by-step reasoning\]"  
     	  
 	def route\_request(self, user\_id, query):  
     	group \= self.get\_user\_group(user\_id)  
     	start\_time \= time.time()  
     	  
     	if group \== "treatment":  
         	response \= self.call\_reasoning\_agent(query)  
     	else:  
         	response \= self.call\_baseline\_model(query)  
     	  
     	response\_time \= time.time() \- start\_time  
     	  
     	\# Log the result for analysis  
     	self.results.append({  
         	"user\_id": user\_id,  
         	"group": group,  
    	     "query": query,  
         	"response": response,  
         	"response\_time": response\_time,  
         	"timestamp": time.time()  
     	})  
     	  
     	return response





def analyze\_ab\_results(self):  
 	control\_results \= \[r for r in self.results if r\["group"\] \== "control"\]  
 	treatment\_results \= \[r for r in self.results if r\["group"\] \== "treatment"\]  
 	  
 	\# Check if we have enough data  
 	if len(control\_results) \== 0 or len(treatment\_results) \== 0:  
     	return {"error": "Need data from both control and treatment groups"}  
 	  
 	\# Calculate key metrics  
 	control\_avg\_time \= sum(r\["response\_time"\] for r in control\_results) / len(control\_results)  
 	treatment\_avg\_time \= sum(r\["response\_time"\] for r in treatment\_results) / len(treatment\_results)  
 	  
 	\# Quality scoring (implement based on your criteria)  
 	control\_quality \= sum(self.score\_response(r\["response"\]) for r in control\_results) / len(control\_results)  
 	treatment\_quality \= sum(self.score\_response(r\["response"\]) for r in treatment\_results) / len(treatment\_results)  
 	  
 	return {  
     	"control\_samples": len(control\_results),  
     	"treatment\_samples": len(treatment\_results),  
     	"control\_avg\_time": control\_avg\_time,  
     	"treatment\_avg\_time": treatment\_avg\_time,  
     	"time\_improvement": control\_avg\_time \- treatment\_avg\_time,  
     	"control\_quality": control\_quality,  
     	"treatment\_quality": treatment\_quality,  
     	"quality\_improvement": treatment\_quality \- control\_quality  
 	}

 def score\_response(self, response):  
 	\# Simple quality scoring \- enhance this based on your specific needs  
 	score \= 0.5  \# Base score  
 	  
 	\# Higher score for longer, more detailed responses  
 	if len(response) \> 50:  
     	score \+= 0.2  
 	  
 	\# Higher score for responses that mention reasoning  
 	if "reasoning" in response.lower():  
     	score \+= 0.3  
     	  
 	return min(score, 1.0)  \# Cap at 1.0

 def print\_results\_table(self):  
 	results \= self.analyze\_ab\_results()  
 	  
 	if "error" in results:  
     	print(f"Error: {results\['error'\]}")  
     	return  
 	  
 	print("\\n" \+ "="\*60)  
 	print("A/B TEST RESULTS SUMMARY")  
 	print("="\*60)  
 	print(f"Control Group Samples: 	{results\['control\_samples'\]:\>8}")  
 	print(f"Treatment Group Samples:   {results\['treatment\_samples'\]:\>8}")  
 	print(f"")  
 	print(f"Average Response Times:")  
 	print(f"  Control:                 {results\['control\_avg\_time'\]:\>8.3f}s")  
 	print(f"  Treatment:               {results\['treatment\_avg\_time'\]:\>8.3f}s")  
 	print(f"  Improvement:             {results\['time\_improvement'\]:\>8.3f}s")  
 	print(f"")  
 	print(f"Quality Scores:")  
 	print(f"  Control:                 {results\['control\_quality'\]:\>8.3f}")  
     print(f"  Treatment:               {results\['treatment\_quality'\]:\>8.3f}")  
 	print(f"  Improvement:             {results\['quality\_improvement'\]:\>8.3f}")  
 	print("="\*60)






from ab\_test\_router import ABTestRouter

 def run\_ab\_test\_simulation():  
 	\# Create the router  
 	router \= ABTestRouter()  
 	  
 	\# Simulate different users and queries  
 	test\_queries \= \[  
     	"What is machine learning?",  
     	"How does artificial intelligence work?",  
     	"Explain neural networks in simple terms.",  
     	"What are the benefits of renewable energy?",  
     	"How do I improve my productivity?",  
     	"What causes climate change?",  
     	"Describe quantum computing.",  
     	"How can I learn programming?",  
     	"What is the future of AI?",  
     	"Explain blockchain technology."  
 	\]  
 	  
 	\# Simulate 100 different users making requests  
 	print("Running A/B test simulation...")  
 	for user\_num in range(1, 101):  
     	user\_id \= f"user\_{user\_num:03d}"  
     	query \= test\_queries\[user\_num % len(test\_queries)\]  
     	  
     	response \= router.route\_request(user\_id, query)  
     	  
     	\# Show progress every 20 requests  
     	if user\_num % 20 \== 0:  
         	print(f"Processed {user\_num} requests...")  
 	  
 	print(f"Simulation complete\! Processed {len(router.results)} total requests.")  
 	  
 	\# Display results  
 	router.print\_results\_table()

 if \_\_name\_\_ \== "\_\_main\_\_":  
 	run\_ab\_test\_simulation()




